import re
import scrapy
import time
import os #for creating directory tree based on year, series name and match name
pref = ''
url_stat = {}
def player_info_url():
    player_dict = {}
    file_count = 0
    url_list = []
    regex = re.compile(r'profiles/[0-9]*')
    for root, dirs, files in os.walk('/home/hhsecond/cricbuz_data/'):
        for file in files:
            current_path = os.path.join(root, file)
            with open(current_path, 'r') as f:
                for line in f:
                    reg = regex.search(line)
                    if reg:
                        player_dict[reg.group()] = ''
    with open('player_url.txt', 'w+') as f:
        for key in player_dict:
            url = 'http://www.cricbuzz.com/' + key
            url_list.append(url)
    return url_list

class player_fetcher(scrapy.Spider):
    ''' this is using to fethc the player information based on the url generated by given function
        second crawler for fetcing the player profile'''
    name = 'player_info'
    allowed_domains = ['cricbuzz.com']
    start_urls = player_info_url()
    def parse(self, response):
        player_name = response.xpath(".//title/text()").extract()
        player_name = player_name[0].split(' Profile')[0]
        with open('/home/hhsecond/cricbuz_data/player_info/' + player_name + '.html', 'wb+') as f:
            f.write(response.body)


class cricbuz_spider(scrapy.Spider):
    """docstring for cricbuz_spide
    Start url given to the spider is the archive page with year list. Functioning of spider can be defined like this:
    It Fetches the link to all the years -->
    From there, it would fetch the link to each series -->
    Series page will have link to each match in that series -->
    Match page will have other 4 subpages with different details which is specifying below
    cricket-scores: Commentary
    cricket-scorecard: Score card
    live-cricket-match-blog: blogs which would probably have details about climate and temperature
    cricket-match-facts: Facts like man of the mathc, total score, ranking etc.
    I would suggest to remove this big doc string, once you understand the concept
    """
    name = 'cricbuz'
    allowed_domains = ['cricbuzz.com']
    start_urls = ['http://www.cricbuzz.com/cricket-scorecard-archives']
    def close(self, reason):
        global url_stat
        with open('total_url.txt', 'w+') as f:
            for key, value in url_stat.items():
                f.write(str(key) + ' ;---; ' + str(value) + '\n')


    def parse(self, response):
        #Fetching year link
        global pref, count
        years = response.xpath('//div[@class="cb-col-33 cb-col cb-col-rt"]/div/a/@href').extract()
        pref = '/'.join(response.url.split('/')[:-1])
        for year in years:
            #time.sleep(20)
            num_year = year.split('/')[-1]
            request = scrapy.Request((pref + year), callback = self.series_parser)
            request.meta['num_year'] = num_year
            yield request

    def series_parser(self, response):
        #Fetching series link
        global pref
        series = response.xpath('//div[@class="cb-col-16 cb-col text-bold cb-srs-cat" and ./text()="International"]/following-sibling::div[1]/div/a/@href').extract()
        for each_series in series:
            #time.sleep(20)
            str_each_series = each_series.split('/')[-2]
            request = scrapy.Request((pref + each_series), callback = self.match_parser)
            request.meta['num_year'] = response.meta['num_year']
            request.meta['str_each_series'] = str_each_series
            yield request

    def match_parser(self, response):
        #Fetching match link
        global pref
        matches = response.xpath('//div[@class="cb-col-60 cb-col cb-srs-mtchs-tm"]/a[1]/@href').extract()
        for match in matches:
            #time.sleep(20)
            request = scrapy.Request((pref + match), callback = self.match_details)
            request.meta['num_year'] = response.meta['num_year']
            request.meta['str_each_series'] = response.meta['str_each_series']
            request.meta['match_name'] = match.split('/')[-1]
            yield request

    def match_details(self, response):
        global url_stat
        #fetching link to all four subpages
        r = response.meta
        url = response.url
        path = str(r['num_year']) + '/' + str(r['str_each_series']) + '/' + str(r['match_name']) + '/'
        current_path = path + 'cricket-scores.html'
        os.makedirs(os.path.dirname(current_path), exist_ok = True)
        with open(current_path, 'wb+') as f:
                f.write(response.body)
        default_val = 'cricket-scores' #default value in the url which we need to replace for getting different pages
        for page in ['cricket-scorecard','live-cricket-match-blog','cricket-match-facts','cricket-scores']:
            #time.sleep(20)
            current_url = url.replace(default_val, page)
            url_stat[current_url] = ''
            current_path = path + page + '.html'
            request = scrapy.Request(current_url, callback = self.writefile)
            request.meta['current_path'] = current_path
            yield request

    def writefile(self, response):
        global url_stat
        url_stat[response.url] = response.status
        #writing source code of those pages into directory tree
        #Directory tree syntax  - c:/year/series/match/one_among_the_4_catagory.html
        current_path = response.meta['current_path']
        os.makedirs(os.path.dirname(current_path), exist_ok = True)
        with open(current_path, 'wb+') as f:
                f.write(response.body)
